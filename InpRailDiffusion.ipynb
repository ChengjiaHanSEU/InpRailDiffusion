{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "informative-behavior",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.layers import Layer\n",
    "from tensorflow.keras import Sequential\n",
    "import tensorflow.keras.layers as nn\n",
    "\n",
    "from tensorflow import einsum\n",
    "from einops import rearrange\n",
    "from einops.layers.tensorflow import Rearrange\n",
    "\n",
    "import math\n",
    "from inspect import isfunction\n",
    "\n",
    "from functools import partial\n",
    "from tqdm import tqdm\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import cv2\n",
    "import pathlib\n",
    "from glob import glob\n",
    "\n",
    "import time\n",
    "from tensorflow.python.data.experimental import AUTOTUNE\n",
    "import tensorflow_addons as tfa\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import skimage\n",
    "from skimage.metrics import structural_similarity\n",
    "import time\n",
    "from skimage import filters, img_as_ubyte,morphology,measure\n",
    "from scipy.ndimage import label\n",
    "import copy\n",
    "import random\n",
    "from skimage import data,filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "headed-business",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################ops#########################\n",
    "# helpers functions\n",
    "def exists(x):\n",
    "    return x is not None\n",
    "\n",
    "def default(val, d):\n",
    "    if exists(val):\n",
    "        return val\n",
    "    return d() if isfunction(d) else d\n",
    "\n",
    "def cycle(dl):\n",
    "    while True:\n",
    "        for data in dl:\n",
    "            yield data\n",
    "\n",
    "def num_to_groups(num, divisor):\n",
    "    groups = num // divisor\n",
    "    remainder = num % divisor\n",
    "    arr = [divisor] * groups\n",
    "    if remainder > 0:\n",
    "        arr.append(remainder)\n",
    "    return arr\n",
    "\n",
    "def normalize_to_neg_one_to_one(img):\n",
    "    return img * 2 - 1\n",
    "\n",
    "def unnormalize_to_zero_to_one(t):\n",
    "    return (t + 1) * 0.5\n",
    "\n",
    "# small helper modules\n",
    "class Identity(Layer):\n",
    "    def __init__(self):\n",
    "        super(Identity, self).__init__()\n",
    "\n",
    "    def call(self, x, training=True):\n",
    "        return tf.identity(x)\n",
    "\n",
    "class EMA(Layer):\n",
    "    def __init__(self, beta=0.995):\n",
    "        super(EMA, self).__init__()\n",
    "        self.beta = beta\n",
    "\n",
    "    @tf.function\n",
    "    def update_model_average(self, old_model, new_model):\n",
    "        for old_weight, new_weight in zip(old_model.weights, new_model.weights):\n",
    "            assert old_weight.shape == new_weight.shape\n",
    "\n",
    "            old_weight.assign(self.update_average(old_weight, new_weight))\n",
    "\n",
    "    def update_average(self, old, new):\n",
    "        if old is None:\n",
    "            return new\n",
    "        return old * self.beta + (1 - self.beta) * new\n",
    "\n",
    "class Residual(Layer):\n",
    "    def __init__(self, fn):\n",
    "        super(Residual, self).__init__()\n",
    "        self.fn = fn\n",
    "\n",
    "    def call(self, x, training=True):\n",
    "        return self.fn(x, training=training) + x\n",
    "\n",
    "class SinusoidalPosEmb(Layer):\n",
    "    def __init__(self, dim, max_positions=10000):\n",
    "        super(SinusoidalPosEmb, self).__init__()\n",
    "        self.dim = dim\n",
    "        self.max_positions = max_positions\n",
    "\n",
    "    def call(self, x, training=True):\n",
    "        x = tf.cast(x, tf.float32)\n",
    "        half_dim = self.dim // 2\n",
    "        emb = math.log(self.max_positions) / (half_dim - 1)\n",
    "        emb = tf.exp(tf.range(half_dim, dtype=tf.float32) * -emb)\n",
    "        emb = x[:, None] * emb[None, :]\n",
    "\n",
    "        emb = tf.concat([tf.sin(emb), tf.cos(emb)], axis=-1)\n",
    "\n",
    "        return emb\n",
    "\n",
    "def Upsample(dim):\n",
    "    return nn.Conv2DTranspose(filters=dim, kernel_size=4, strides=2, padding='SAME')\n",
    "\n",
    "def Downsample(dim):\n",
    "    return nn.Conv2D(filters=dim, kernel_size=4, strides=2, padding='SAME')\n",
    "\n",
    "class LayerNorm(Layer):\n",
    "    def __init__(self, dim, eps=1e-5):\n",
    "        super(LayerNorm, self).__init__()\n",
    "        self.eps = eps\n",
    "\n",
    "        self.g = tf.Variable(tf.ones([1, 1, 1, dim]))\n",
    "        self.b = tf.Variable(tf.zeros([1, 1, 1, dim]))\n",
    "\n",
    "    def call(self, x, training=True):\n",
    "        var = tf.math.reduce_variance(x, axis=-1, keepdims=True)\n",
    "        mean = tf.reduce_mean(x, axis=-1, keepdims=True)\n",
    "\n",
    "        x = (x - mean) / tf.sqrt((var + self.eps)) * self.g + self.b\n",
    "        return x\n",
    "\n",
    "class PreNorm(Layer):\n",
    "    def __init__(self, dim, fn):\n",
    "        super(PreNorm, self).__init__()\n",
    "        self.fn = fn\n",
    "        self.norm = LayerNorm(dim)\n",
    "\n",
    "    def call(self, x, training=True):\n",
    "        x = self.norm(x)\n",
    "        return self.fn(x)\n",
    "\n",
    "class SiLU(Layer):\n",
    "    def __init__(self):\n",
    "        super(SiLU, self).__init__()\n",
    "\n",
    "    def call(self, x, training=True):\n",
    "        return x * tf.nn.sigmoid(x)\n",
    "\n",
    "def gelu(x, approximate=False):\n",
    "    if approximate:\n",
    "        coeff = tf.cast(0.044715, x.dtype)\n",
    "        return 0.5 * x * (1.0 + tf.tanh(0.7978845608028654 * (x + coeff * tf.pow(x, 3))))\n",
    "    else:\n",
    "        return 0.5 * x * (1.0 + tf.math.erf(x / tf.cast(1.4142135623730951, x.dtype)))\n",
    "\n",
    "class GELU(Layer):\n",
    "    def __init__(self, approximate=False):\n",
    "        super(GELU, self).__init__()\n",
    "        self.approximate = approximate\n",
    "\n",
    "    def call(self, x, training=True):\n",
    "        return gelu(x, self.approximate)\n",
    "\n",
    "def linear_beta_schedule(timesteps):\n",
    "    scale = 1000 / timesteps\n",
    "    beta_start = scale * 0.0001\n",
    "    beta_end = scale * 0.02\n",
    "\n",
    "    return tf.cast(tf.linspace(beta_start, beta_end, timesteps), tf.float32)\n",
    "\n",
    "def cosine_beta_schedule(timesteps, s = 0.008):\n",
    "    \"\"\"\n",
    "    cosine schedule\n",
    "    as proposed in https://openreview.net/forum?id=-NEXDKk8gZ\n",
    "    \"\"\"\n",
    "    steps = timesteps + 1\n",
    "    x = tf.cast(tf.linspace(0, timesteps, steps), tf.float32)\n",
    "\n",
    "    alphas_cumprod = tf.cos(((x / timesteps) + s) / (1 + s) * math.pi * 0.5) ** 2\n",
    "    alphas_cumprod = alphas_cumprod / alphas_cumprod[0]\n",
    "    betas = 1 - (alphas_cumprod[1:] / alphas_cumprod[:-1])\n",
    "\n",
    "    return tf.clip_by_value(betas, 0, 0.999)\n",
    "\n",
    "def extract(x, t):\n",
    "    return tf.gather(x, t)[:, None, None, None]\n",
    "\n",
    "def Filter(image,model=\"BLUR\"): #均值模糊\n",
    "    if model == \"conv2D\":\n",
    "        kernel = np.ones((3, 3)) / 9\n",
    "        dst = cv2.filter2D(image, -1, kernel)\n",
    "    if model == \"BLUR\":\n",
    "        dst = cv2.blur(image, (10, 10))\n",
    "    if model == \"Guass\":\n",
    "        dst = cv2.GaussianBlur(image, (0, 0), 2)\n",
    "    return dst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60c2d8c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"VisionMambaBlock module.\"\"\"\n",
    "from einops import rearrange, repeat\n",
    "from math import ceil\n",
    "class RMSNorm(tf.keras.layers.Layer):\n",
    "    def __init__(self, eps = 1e-5):\n",
    "        super(RMSNorm, self).__init__()\n",
    "        self.eps = eps\n",
    "    def build(self, input_shape):\n",
    "        self.weight = self.add_weight(shape = (input_shape[-1],), dtype = tf.float32, trainable = True, initializer = tf.keras.initializers.Constant(1.), name = 'weight')\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape\n",
    "    def call(self, inputs):\n",
    "        stddev = tf.math.maximum(tf.math.sqrt(tf.math.reduce_mean(inputs ** 2, axis = -1, keepdims = True)), self.eps)\n",
    "        results = inputs / stddev\n",
    "        results = results * self.weight\n",
    "        return results\n",
    "    def get_config(self):\n",
    "        config = super(RMSNorm, self).get_config()\n",
    "        config['eps'] = self.eps\n",
    "        return config\n",
    "    @classmethod\n",
    "    def from_config(cls, config):\n",
    "        return cls(**config)\n",
    "    \n",
    "def selective_scan(u, delta, A, B, C, D):\n",
    "    dA = tf.einsum('bld,dn->bldn', delta, A) # first step of A_bar = exp(ΔA), i.e., ΔA\n",
    "    dB_u = tf.einsum('bld,bld,bln->bldn', delta, u, B)\n",
    "    dA_cumsum = tf.pad(\n",
    "        dA[:, 1:], [[0, 0], [1, 1], [0, 0], [0, 0]])[:, 1:, :, :]\n",
    "    dA_cumsum = tf.reverse(dA_cumsum, axis=[1])  # Flip along axis 1\n",
    "    # Cumulative sum along all the input tokens, parallel prefix sum, calculates dA for all the input tokens parallely\n",
    "    dA_cumsum = tf.math.cumsum(dA_cumsum, axis=1)  \n",
    "    dA_cumsum = tf.exp(dA_cumsum)  # second step of A_bar = exp(ΔA), i.e., exp(ΔA)\n",
    "    dA_cumsum = tf.reverse(dA_cumsum, axis=[1])  # Flip back along axis 1\n",
    "    x = dB_u * dA_cumsum\n",
    "    x = tf.math.cumsum(x, axis=1)/(dA_cumsum + 1e-12) # 1e-12 to avoid division by 0\n",
    "    y = tf.einsum('bldn,bln->bld', x, C)\n",
    "    return y + u * D \n",
    "\n",
    "class SSM(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, expand = 2, d_state = 16, bias = False):\n",
    "        super(SSM, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.expand = expand\n",
    "        self.d_state = d_state\n",
    "        self.bias = bias\n",
    "        self.dt_rank = ceil(self.d_model / 16)\n",
    "    def build(self, input_shape):\n",
    "        self.x_proj_weight = self.add_weight(shape = (self.d_model * self.expand, self.dt_rank + 2 * self.d_state), dtype = tf.float32, trainable = True, name = 'x_proj_weight')\n",
    "        if self.bias:\n",
    "            self.x_proj_bias = self.add_weight(shape = (self.dt_rank + 2 * self.d_state), dtype = tf.float32, trainable = True, name = 'x_proj_bias')\n",
    "        self.dt_proj_weight = self.add_weight(shape = (self.dt_rank, self.expand * self.d_model), dtype = tf.float32, trainable = True, name = 'dt_proj_wei9ght')\n",
    "        self.dt_proj_bias = self.add_weight(shape = (self.expand * self.d_model,), dtype = tf.float32, trainable = True, name = 'dt_proj_bias')\n",
    "        self.A_log = self.add_weight(shape = (self.expand * self.d_model, self.d_state), dtype = tf.float32, trainable = True, name = 'A_log')\n",
    "        self.A_log.assign(tf.math.log(tf.tile(tf.expand_dims(tf.range(1, self.d_state + 1, dtype = tf.float32), axis = 0), (self.expand * self.d_model, 1))))\n",
    "        self.D = self.add_weight(shape = (self.expand * self.d_model,), dtype = tf.float32, trainable = True, initializer = tf.keras.initializers.Constant(1.), name = 'D')\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], input_shape[1], self.d_model * self.expand)\n",
    "    def call(self, x):\n",
    "        # x.shape = (batch, seq_len, d_model * expand)\n",
    "        x_dbl = tf.linalg.matmul(x, self.x_proj_weight) # x_dbl.shape = (batch, seq_len, dt_rank + 2 * d_state)\n",
    "        if self.bias:\n",
    "            x_dbl = x_dbl + self.x_proj_bias\n",
    "        delta, B, C = tf.split(x_dbl, [self.dt_rank, self.d_state, self.d_state], axis = -1)\n",
    "        # delta.shape = (batch, seq_len, dt_rank)\n",
    "        # B.shape = (batch, seq_len, d_state)\n",
    "        # C.shape = (batch, seq_len, d_state)\n",
    "        delta = tf.math.softplus(tf.linalg.matmul(delta, self.dt_proj_weight) + self.dt_proj_bias) # delta.shape = (batch, seq_len, expand * d_model)\n",
    "        # selective scan\n",
    "        # state(t+1) = A state(t) + B x(t) # B is input gate\n",
    "        # y(t)   = C state(t) + D x(t) # C is output gate\n",
    "        A = -tf.exp(self.A_log) # A.shape = (expand * d_model, d_state)\n",
    "        y = selective_scan(x, delta,A,B,C,self.D)\n",
    "        return y\n",
    "    def get_config(self):\n",
    "        config = super(SSM, self).get_config()\n",
    "        config['d_model'] = self.d_model\n",
    "        config['expand'] = self.expand\n",
    "        config['d_state'] = self.d_state\n",
    "        config['bias'] = self.bias\n",
    "        return config\n",
    "    @classmethod\n",
    "    def from_config(cls, config):\n",
    "        return cls(**config)\n",
    "        \n",
    "class MambaBlock(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, expand = 2, bias = False, d_conv = 4, conv_bias = True, d_state = 16):\n",
    "        super(MambaBlock, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.expand = expand\n",
    "        self.d_state = d_state\n",
    "        self.bias = bias\n",
    "        self.dt_rank = ceil(self.d_model / 16)\n",
    "        self.d_conv = d_conv\n",
    "        self.conv_bias = conv_bias\n",
    "        self.fliter = d_model*expand\n",
    "    def call(self,x):\n",
    "        x_and_res = tf.keras.layers.Dense(2 * self.expand * self.d_model, use_bias = self.bias)(x) # results.shape = (batch, seq_len, 2 * expand * d_model)\n",
    "        x, res = tf.keras.layers.Lambda(lambda x: tf.split(x, 2, axis = -1))(x_and_res) # x.shape = (batch, seq_len, expand * d_model)\n",
    "        # spatial & channel mixing\n",
    "        x = tf.keras.layers.Conv1D(self.fliter, kernel_size = self.d_conv, padding = 'same', use_bias = self.conv_bias, activation = tf.keras.activations.swish)(x) # x.shape = (batch, seq_len, expand * d_model)\n",
    "        # selective state space model\n",
    "        y = SSM(self.d_model, self.expand, self.d_state, self.bias)(x) # y.shape = (batch, seq_len, d_model * expand)\n",
    "        # NOTE: borrowing idea of Swish gated linear unit (SwiGLU)\n",
    "        # this layer gates ssm results with swish layer as well. it can be called as swish gated selective state space model (SwiSSM)\n",
    "        y = tf.keras.layers.Lambda(lambda x: x[0] * tf.nn.silu(x[1]))([y, res])\n",
    "        outputs = tf.keras.layers.Dense(self.d_model, use_bias = self.bias)(y)\n",
    "        return outputs\n",
    "# Pair\n",
    "def pair(t):\n",
    "    return t if isinstance(t, tuple) else (t, t)\n",
    "\n",
    "class VisionEncoderMambaBlock(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    VisionMambaBlock is a module that implements the Mamba block from the paper\n",
    "    Vision Mamba: Efficient Visual Representation Learning with Bidirectional\n",
    "    State Space Model\n",
    "\n",
    "    Args:\n",
    "        dim (int): The input dimension of the input tensor.\n",
    "        dt_rank (int): The rank of the state space model.\n",
    "        dim_inner (int): The dimension of the inner layer of the\n",
    "            multi-head attention.\n",
    "        d_state (int): The dimension of the state space model.\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "        dim: int,\n",
    "        dim_inner: int = 4,\n",
    "        d_state: int = 16,\n",
    "        patch_size_H: int = 16,\n",
    "        patch_size_L: int = 16,\n",
    "    ):\n",
    "        super(VisionEncoderMambaBlock, self).__init__()\n",
    "        self.dim = dim\n",
    "        self.dim_inner = dim_inner\n",
    "        self.d_state = d_state\n",
    "        \n",
    "        self.patch_height = patch_size_H\n",
    "        self.patch_width  = patch_size_L\n",
    "        self.x0_conv1d = tf.keras.layers.SeparableConv1D(dim, kernel_size=1)\n",
    "        self.silu = SiLU()\n",
    "        self.ssmx =  MambaBlock(d_model = dim, expand = dim_inner, d_state = d_state)\n",
    "\n",
    "    def call(self, x, training=True):\n",
    "        # Patch embedding\n",
    "        b, H, W, c= x.shape\n",
    "        x = rearrange(x, \"b (h p1) (w p2) c -> b (h w) (p1 p2 c)\", p1=self.patch_height, p2 =self.patch_width)\n",
    "        x0 = self.x0_conv1d(x)\n",
    "        x = self.ssmx(x0)\n",
    "        x = self.silu(x)+x0\n",
    "        x = tf.keras.layers.Dense(self.patch_height*self.patch_width*c,activation=None)(x)\n",
    "        x = rearrange(x, \"b (h w) (p1 p2 c)->b (h p1) (w p2) c\", h = H//self.patch_height, p1=self.patch_height, p2 =self.patch_width)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imposed-story",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################util#########################\n",
    "class Image_data:\n",
    "\n",
    "    def __init__(self, img_size, dataset_path):\n",
    "        self.img_size = img_size\n",
    "        self.dataset_path = dataset_path\n",
    "\n",
    "\n",
    "    def image_processing(self, filename):\n",
    "\n",
    "        x = tf.io.read_file(filename)\n",
    "        x_decode = tf.image.decode_jpeg(x, channels=3, dct_method='INTEGER_ACCURATE')\n",
    "        img = tf.image.resize(x_decode, [self.img_size, self.img_size], antialias=True, method=tf.image.ResizeMethod.BICUBIC)\n",
    "        img = preprocess_fit_train_image(img)\n",
    "\n",
    "        return img\n",
    "\n",
    "    def preprocess(self):\n",
    "\n",
    "        self.train_images = glob(os.path.join(self.dataset_path, '*.png')) + glob(os.path.join(self.dataset_path, '*.jpg'))\n",
    "\n",
    "def adjust_dynamic_range(images, range_in, range_out, out_dtype):\n",
    "    scale = (range_out[1] - range_out[0]) / (range_in[1] - range_in[0])\n",
    "    bias = range_out[0] - range_in[0] * scale\n",
    "    images = images * scale + bias\n",
    "    images = tf.clip_by_value(images, range_out[0], range_out[1])\n",
    "    images = tf.cast(images, dtype=out_dtype)\n",
    "    return images\n",
    "\n",
    "def random_flip_left_right(images):\n",
    "    s = tf.shape(images)\n",
    "    mask = tf.random.uniform([1, 1, 1], 0.0, 1.0)\n",
    "    mask = tf.tile(mask, [s[0], s[1], s[2]]) # [h, w, c]\n",
    "    images = tf.where(mask < 0.5, images, tf.reverse(images, axis=[1]))\n",
    "    return images\n",
    "\n",
    "def preprocess_fit_train_image(images):\n",
    "    images = adjust_dynamic_range(images, range_in=(0.0, 255.0), range_out=(-1.0, 1.0), out_dtype=tf.dtypes.float32)\n",
    "    images = random_flip_left_right(images)\n",
    "    # images = tf.transpose(images, [2, 0, 1])\n",
    "\n",
    "    return images\n",
    "\n",
    "def preprocess_image(images):\n",
    "    images = adjust_dynamic_range(images, range_in=(0.0, 255.0), range_out=(-1.0, 1.0), out_dtype=tf.dtypes.float32)\n",
    "    # images = tf.transpose(images, [2, 0, 1])\n",
    "\n",
    "    return images\n",
    "\n",
    "def postprocess_images(images):\n",
    "    images = adjust_dynamic_range(images, range_in=(-1.0, 1.0), range_out=(0.0, 255.0), out_dtype=tf.dtypes.float32)\n",
    "    # images = tf.transpose(images, [0, 2, 3, 1])\n",
    "    images = tf.cast(images, dtype=tf.dtypes.uint8)\n",
    "    return images\n",
    "\n",
    "def load_images(image_path, img_width, img_height, img_channel):\n",
    "\n",
    "    # from PIL import Image\n",
    "    if img_channel == 1 :\n",
    "        img = cv2.imread(image_path, flags=cv2.IMREAD_GRAYSCALE)\n",
    "    else :\n",
    "        img = cv2.imread(image_path, flags=cv2.IMREAD_COLOR)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # img = cv2.resize(img, dsize=(img_width, img_height))\n",
    "    img = tf.image.resize(img, [img_height, img_width], antialias=True, method=tf.image.ResizeMethod.BICUBIC)\n",
    "    img = preprocess_image(img)\n",
    "\n",
    "    if img_channel == 1 :\n",
    "        img = np.expand_dims(img, axis=0)\n",
    "        img = np.expand_dims(img, axis=-1)\n",
    "    else :\n",
    "        img = np.expand_dims(img, axis=0)\n",
    "\n",
    "    return img\n",
    "\n",
    "def save_images(images, size, image_path):\n",
    "    # size = [height, width]\n",
    "    return imsave(postprocess_images(images), size, image_path)\n",
    "\n",
    "def imsave(images, size, path):\n",
    "    images = merge(images, size)\n",
    "    images = cv2.cvtColor(images.astype('uint8'), cv2.COLOR_RGB2BGR)\n",
    "\n",
    "    return cv2.imwrite(path, images)\n",
    "\n",
    "def merge(images, size):\n",
    "    h, w = images.shape[1], images.shape[2]\n",
    "    img = np.zeros((h * size[0], w * size[1], 3))\n",
    "    for idx, image in enumerate(images):\n",
    "        i = idx % size[1]\n",
    "        j = idx // size[1]\n",
    "        img[h*j:h*(j+1), w*i:w*(i+1), :] = image\n",
    "\n",
    "    return img\n",
    "\n",
    "def str2bool(x):\n",
    "    return x.lower() in ('true')\n",
    "\n",
    "def check_folder(log_dir):\n",
    "    if not os.path.exists(log_dir):\n",
    "        os.makedirs(log_dir)\n",
    "    return log_dir\n",
    "\n",
    "def automatic_gpu_usage() :\n",
    "    gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "    if gpus:\n",
    "        try:\n",
    "            # Currently, memory growth needs to be the same across GPUs\n",
    "            for gpu in gpus:\n",
    "                tf.config.experimental.set_memory_growth(gpu, True)\n",
    "            logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "            print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "        except RuntimeError as e:\n",
    "            # Memory growth must be set before GPUs have been initialized\n",
    "            print(e)\n",
    "\n",
    "def multi_gpu_loss(x, global_batch_size):\n",
    "    ndim = len(x.shape)\n",
    "    no_batch_axis = list(range(1, ndim))\n",
    "    x = tf.reduce_mean(x, axis=no_batch_axis)\n",
    "    x = tf.reduce_sum(x) / global_batch_size\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "numeric-committee",
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################layers############################\n",
    "# building block modules\n",
    "class Block(Layer):\n",
    "    def __init__(self, dim, groups=8):\n",
    "        super(Block, self).__init__()\n",
    "        self.proj = nn.Conv2D(dim, kernel_size=3, strides=1, padding='SAME')\n",
    "        self.norm = tfa.layers.GroupNormalization(groups, epsilon=1e-05)\n",
    "        self.act = SiLU() # x * sigmoid(x)\n",
    "\n",
    "    def call(self, x, scale_shift=None, training=True):\n",
    "        x = self.proj(x)\n",
    "        x = self.norm(x, training=training)\n",
    "\n",
    "        if exists(scale_shift):\n",
    "            scale, shift = scale_shift\n",
    "            x = x * (scale + 1) + shift\n",
    "\n",
    "        x = self.act(x)\n",
    "        return x\n",
    "\n",
    "class ResnetBlock(Layer):\n",
    "    def __init__(self, dim, dim_out, time_emb_dim=None, groups=8):\n",
    "        super(ResnetBlock, self).__init__()\n",
    "\n",
    "        self.mlp = Sequential([\n",
    "            SiLU(),\n",
    "            nn.Dense(units=dim_out * 2)\n",
    "        ]) if exists(time_emb_dim) else None\n",
    "\n",
    "        self.block1 = Block(dim_out, groups=groups)\n",
    "        self.block2 = Block(dim_out, groups=groups)\n",
    "        self.res_conv = nn.Conv2D(filters=dim_out, kernel_size=1, strides=1) if dim != dim_out else Identity()\n",
    "\n",
    "    def call(self, x, time_emb=None, training=True):\n",
    "        scale_shift = None\n",
    "        if exists(self.mlp) and exists(time_emb):\n",
    "            time_emb = self.mlp(time_emb)\n",
    "            time_emb = rearrange(time_emb, 'b c -> b 1 1 c')\n",
    "            scale_shift = tf.split(time_emb, num_or_size_splits=2, axis=-1)\n",
    "\n",
    "        h = self.block1(x, scale_shift=scale_shift, training=training)\n",
    "        h = self.block2(h, training=training)\n",
    "\n",
    "        return h + self.res_conv(x)\n",
    "\n",
    "class LinearAttention(Layer):\n",
    "    def __init__(self, dim, heads=4, dim_head=32):\n",
    "        super(LinearAttention, self).__init__()\n",
    "        self.scale = dim_head ** -0.5\n",
    "        self.heads = heads\n",
    "        self.hidden_dim = dim_head * heads\n",
    "\n",
    "        self.attend = nn.Softmax()\n",
    "        self.to_qkv = nn.Conv2D(filters=self.hidden_dim * 3, kernel_size=1, strides=1, use_bias=False)\n",
    "\n",
    "        self.to_out = Sequential([\n",
    "            nn.Conv2D(filters=dim, kernel_size=1, strides=1),\n",
    "            LayerNorm(dim)\n",
    "        ])\n",
    "\n",
    "    def call(self, x, training=True):\n",
    "        b, h, w, c = x.shape\n",
    "        qkv = self.to_qkv(x)\n",
    "        qkv = tf.split(qkv, num_or_size_splits=3, axis=-1)\n",
    "        q, k, v = map(lambda t: rearrange(t, 'b x y (h c) -> b h c (x y)', h=self.heads), qkv)\n",
    "\n",
    "        q = tf.nn.softmax(q, axis=-2)\n",
    "        k = tf.nn.softmax(k, axis=-1)\n",
    "\n",
    "        q = q * self.scale\n",
    "        context = einsum('b h d n, b h e n -> b h d e', k, v)\n",
    "\n",
    "        out = einsum('b h d e, b h d n -> b h e n', context, q)\n",
    "        out = rearrange(out, 'b h c (x y) -> b x y (h c)', h=self.heads, x=h, y=w)\n",
    "        out = self.to_out(out, training=training)\n",
    "\n",
    "        return out\n",
    "\n",
    "class Attention(Layer):\n",
    "    def __init__(self, dim, heads=4, dim_head=32):\n",
    "        super(Attention, self).__init__()\n",
    "        self.scale = dim_head ** -0.5\n",
    "        self.heads = heads\n",
    "        self.hidden_dim = dim_head * heads\n",
    "\n",
    "        self.to_qkv = nn.Conv2D(filters=self.hidden_dim * 3, kernel_size=1, strides=1, use_bias=False)\n",
    "        self.to_out = nn.Conv2D(filters=dim, kernel_size=1, strides=1)\n",
    "\n",
    "    def call(self, x, training=True):\n",
    "        b, h, w, c = x.shape\n",
    "        qkv = self.to_qkv(x)\n",
    "        qkv = tf.split(qkv, num_or_size_splits=3, axis=-1)\n",
    "        q, k, v = map(lambda t: rearrange(t, 'b x y (h c) -> b h c (x y)', h=self.heads), qkv)\n",
    "        q = q * self.scale\n",
    "\n",
    "        sim = einsum('b h d i, b h d j -> b h i j', q, k)\n",
    "        sim_max = tf.stop_gradient(tf.expand_dims(tf.argmax(sim, axis=-1), axis=-1))\n",
    "        sim_max = tf.cast(sim_max, tf.float32)\n",
    "        sim = sim - sim_max\n",
    "        attn = tf.nn.softmax(sim, axis=-1)\n",
    "\n",
    "        out = einsum('b h i j, b h d j -> b h i d', attn, v)\n",
    "        out = rearrange(out, 'b h (x y) d -> b x y (h d)', x = h, y = w)\n",
    "        out = self.to_out(out, training=training)\n",
    "\n",
    "        return out\n",
    "\n",
    "class MLP(Layer):\n",
    "    def __init__(self, hidden_dim):\n",
    "        super(MLP, self).__init__()\n",
    "        self.net = Sequential([\n",
    "            Rearrange('... -> ... 1'), # expand_dims(axis=-1)\n",
    "            nn.Dense(units=hidden_dim),\n",
    "            GELU(),\n",
    "            LayerNorm(hidden_dim),\n",
    "            nn.Dense(units=hidden_dim),\n",
    "            GELU(),\n",
    "            LayerNorm(hidden_dim),\n",
    "            nn.Dense(units=hidden_dim),\n",
    "        ])\n",
    "\n",
    "    def call(self, x, training=True):\n",
    "        return self.net(x, training=training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "presidential-carbon",
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################network#####################################\n",
    "class Unet(Model):\n",
    "    def __init__(self,\n",
    "                 dim=64,\n",
    "                 init_dim=None,\n",
    "                 out_dim=None,\n",
    "                 dim_mults=(1, 2, 4, 8),\n",
    "                 channels=1,\n",
    "                 resnet_block_groups=8,\n",
    "                 learned_variance=False,\n",
    "                 sinusoidal_cond_mlp=True\n",
    "                 ):\n",
    "        super(Unet, self).__init__()\n",
    "\n",
    "        # determine dimensions\n",
    "        self.channels = channels\n",
    "\n",
    "        init_dim = default(init_dim, dim // 3 * 2)\n",
    "        self.init_conv = nn.Conv2D(filters=init_dim, kernel_size=7, strides=1, padding='SAME')\n",
    "\n",
    "        dims = [init_dim, *map(lambda m: dim * m, dim_mults)]\n",
    "        in_out = list(zip(dims[:-1], dims[1:]))\n",
    "\n",
    "        block_klass = partial(ResnetBlock, groups = resnet_block_groups)\n",
    "\n",
    "        # time embeddings\n",
    "        time_dim = dim * 4\n",
    "        self.sinusoidal_cond_mlp = sinusoidal_cond_mlp\n",
    "\n",
    "        if sinusoidal_cond_mlp:\n",
    "            self.time_mlp = Sequential([\n",
    "                SinusoidalPosEmb(dim),\n",
    "                nn.Dense(units=time_dim),\n",
    "                GELU(),\n",
    "                nn.Dense(units=time_dim)\n",
    "            ])\n",
    "        else:\n",
    "            self.time_mlp = MLP(time_dim)\n",
    "\n",
    "        # layers\n",
    "        self.downs = []\n",
    "        self.ups = []\n",
    "        num_resolutions = len(in_out)\n",
    "\n",
    "        for ind, (dim_in, dim_out) in enumerate(in_out):\n",
    "            is_last = ind >= (num_resolutions - 1)\n",
    "\n",
    "            self.downs.append([\n",
    "                block_klass(dim_in, dim_out, time_emb_dim=time_dim),\n",
    "                block_klass(dim_out, dim_out, time_emb_dim=time_dim),\n",
    "                Residual(PreNorm(dim_out,VisionEncoderMambaBlock(dim_out,4,16,patch_size_H=5,patch_size_L=4))),\n",
    "                Downsample(dim_out) if not is_last else Identity()\n",
    "            ])\n",
    "\n",
    "        mid_dim = dims[-1]\n",
    "        self.mid_block1 = block_klass(mid_dim, mid_dim, time_emb_dim=time_dim)\n",
    "        self.mid_attn = Residual(PreNorm(mid_dim, Attention(mid_dim)))\n",
    "        self.mid_block2 = block_klass(mid_dim, mid_dim, time_emb_dim=time_dim)\n",
    "\n",
    "        for ind, (dim_in, dim_out) in enumerate(reversed(in_out[1:])):\n",
    "            is_last = ind >= (num_resolutions - 1)\n",
    "\n",
    "            self.ups.append([\n",
    "                block_klass(dim_out * 2, dim_in, time_emb_dim=time_dim),\n",
    "                block_klass(dim_in, dim_in, time_emb_dim=time_dim),\n",
    "                Residual(PreNorm(dim_in,VisionEncoderMambaBlock(dim_in,4,16,patch_size_H=5,patch_size_L=4))),\n",
    "                Upsample(dim_in) if not is_last else Identity()\n",
    "            ])\n",
    "\n",
    "        default_out_dim = channels * (1 if not learned_variance else 2)\n",
    "        self.out_dim = default(out_dim, default_out_dim)\n",
    "\n",
    "        self.final_conv = Sequential([\n",
    "            block_klass(dim * 2, dim),\n",
    "            nn.Conv2D(filters=self.out_dim, kernel_size=1, strides=1)\n",
    "        ])\n",
    "\n",
    "    def call(self, x, time=None, training=True, **kwargs):\n",
    "        x = self.init_conv(x)\n",
    "        t = self.time_mlp(time)\n",
    "\n",
    "        h = []\n",
    "\n",
    "        for block1, block2, attn, downsample in self.downs:\n",
    "            x = block1(x, t)\n",
    "            x = block2(x, t)\n",
    "            x = attn(x)\n",
    "            h.append(x)\n",
    "            x = downsample(x)\n",
    "\n",
    "        x = self.mid_block1(x, t)\n",
    "        x = self.mid_attn(x)\n",
    "        x = self.mid_block2(x, t)\n",
    "\n",
    "        for block1, block2, attn, upsample in self.ups:\n",
    "            x = tf.concat([x, h.pop()], axis=-1)\n",
    "            x = block1(x, t)\n",
    "            x = block2(x, t)\n",
    "            x = attn(x)\n",
    "            x = upsample(x)\n",
    "\n",
    "        x = tf.concat([x, h.pop()], axis=-1)\n",
    "        x = self.final_conv(x)\n",
    "        return x\n",
    "    \n",
    "    \n",
    "class InpRailDiffusion(Model): \n",
    "    def __init__(self, image_size):\n",
    "        super(InpRailDiffusion, self).__init__()\n",
    "#         self.small_size = random.choice([10,20,40])\n",
    "#         self.timesteps = int(int(200/self.small_size)*int(160/self.small_size)/4)\n",
    "        self.image_size = image_size\n",
    "    def sample_timesteps(self, n, small_size,step=1, t_cut=None):\n",
    "        timesteps = int(int(200/small_size)*int(160/small_size)/2//step)\n",
    "        if t_cut==None:\n",
    "            sample_t = tf.random.uniform(shape=[n], minval=1, maxval=timesteps + 1, dtype=tf.int32)\n",
    "        else:\n",
    "            sample_t = tf.random.uniform(shape=[n], minval=1, maxval=t_cut, dtype=tf.int32)\n",
    "        return sample_t\n",
    "\n",
    "    def noise_images(self, x, t, step=1, size=None, Seed=None, path_number=None, show=False): # forward process q ##########正向disfussion过程###############\n",
    "        xt =[]\n",
    "        if size==None:\n",
    "            small_size=self.small_size \n",
    "        else:\n",
    "            small_size=size\n",
    "        All_grid = [k for k in range(1,int(int(200/small_size)*int(160/small_size))+1)]\n",
    "        if Seed!=None:\n",
    "            np.random.seed(Seed)\n",
    "        np.random.shuffle(All_grid)\n",
    "        Noisepath=[]\n",
    "        for i in range(2):\n",
    "            Noisepath.append(All_grid[int(len(All_grid)/2)*i:int(len(All_grid)/2)*(i+1)])\n",
    "        if  path_number==None:\n",
    "            tem_list = Noisepath[random.choice([0,1])]\n",
    "        else:\n",
    "            tem_list = Noisepath[path_number]\n",
    "        for batch in range(len(x)):\n",
    "            tem_x = copy.deepcopy(x[batch])\n",
    "            tem_t= np.max(t[batch])*step #随便取个值\n",
    "            new_tem_t = np.clip(tem_t,tem_t,len(tem_list)) #限制\n",
    "            for j in range(new_tem_t//step):\n",
    "                for s in range(step):\n",
    "                    tem_L = int(tem_list[j*step+s]/160*small_size)\n",
    "                    if (tem_list[j*step+s]-tem_L*160/small_size)>0:\n",
    "                        tem_W = int(tem_list[j*step+s]-tem_L*160/small_size)-1\n",
    "                    else:\n",
    "                        tem_L = tem_L-1\n",
    "                        tem_W = int(160/small_size)-1\n",
    "                    tem_x[tem_L*small_size:(tem_L+1)*small_size,tem_W*small_size:(tem_W+1)*small_size,:]=0\n",
    "                    if (batch == 0) & (show==True):\n",
    "                        plt.imshow(tem_x)\n",
    "                        plt.show()\n",
    "            xt.append(tem_x) \n",
    "        xt = np.array(xt)\n",
    "        return xt #t时刻噪声图\n",
    "\n",
    "    def sample(self, model, n,  step=1, size=None, Seed=None, start_x=None, start_t=None, path_number=None,show=False): # reverse process p ##########反向disfussion过程###############\n",
    "        if start_x is None:\n",
    "            x = tf.random.normal(shape=[n, self.image_size[0], self.image_size[1], 1]) \n",
    "        else:\n",
    "            x = start_x\n",
    "            \n",
    "        timesteps = int(int(200/size)*int(160/size)/2//step)\n",
    "\n",
    "        for i in tqdm(reversed(range(1, self.timesteps if start_t is None else (start_t+1))), desc='sampling loop time step', total=timesteps):\n",
    "            ####这里不随机，按照设计的轨迹加mask#####\n",
    "            t = tf.ones(n, dtype=tf.int32) * i #从最后时刻倒着往前推\n",
    "            x0_predicted =  model(x, t) #U-Net输出预测上一帧原始图像\n",
    "            x0_predicted = np.array(x0_predicted)\n",
    "            Dt = self.noise_images(x0_predicted,t, step, size,Seed,path_number=path_number)\n",
    "            if i==1:\n",
    "                Dt_1 = x0_predicted\n",
    "            else:\n",
    "                Dt_1 = self.noise_images(x0_predicted, tf.ones(n, dtype=tf.int32) * (i-1),step, size,Seed, path_number=path_number)\n",
    "            tem = Dt_1 - Dt \n",
    "            x = x + tem\n",
    "            if show==True:\n",
    "                plt.imshow(x[0])\n",
    "                plt.show()\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38210409",
   "metadata": {},
   "outputs": [],
   "source": [
    "automatic_gpu_usage()#分配GPU\n",
    "\n",
    "##########加载数据#############\n",
    "Type_I_path_Img = r'D:\\AI in NTU\\Rail data\\RSDDs\\Type-I RSDDs dataset\\Train\\Rail surface images'\n",
    "\n",
    "paths =[]\n",
    "for p in os.listdir(Type_I_path_Img):\n",
    "    paths.append(Type_I_path_Img +'\\\\'+ p)\n",
    "\n",
    "#print(paths)\n",
    "#paths_count = len(paths)\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "#创建图片路径及其数字标签的dataset\n",
    "db_train= tf.data.Dataset.from_tensor_slices(paths)\n",
    "db_train = db_train.shuffle(buffer_size=8,seed=2023)\n",
    "db_train = db_train.batch(BATCH_SIZE)\n",
    "\n",
    "def load_image(path):\n",
    "    path = str(path)[12:-26].replace(\"\\\\\\\\\",\"/\")\n",
    "    image = cv2.imdecode(np.fromfile(path,dtype=np.uint8),-1)\n",
    "    image =np.array(image)[:,:,np.newaxis]\n",
    "    return image\n",
    "\n",
    "\"\"\" Network \"\"\"\n",
    "unet = Unet(dim=32)\n",
    "diffusion = InpRailDiffusion((200, 160))\n",
    "\n",
    "\"\"\" Finalize model (build) \"\"\"\n",
    "test_images = np.ones([1, 200, 160, 1])\n",
    "small_size = random.choice([10,20,40])\n",
    "Step=5\n",
    "test_t = diffusion.sample_timesteps(n=test_images.shape[0],small_size=small_size,step=Step)\n",
    "_ = unet(test_images, test_t)\n",
    "\n",
    "\"\"\" Optimizer \"\"\"\n",
    "optimizer = tf.keras.optimizers.Adam(1e-4)\n",
    "#unet.load_weights(\"./Diffusion-Paints-blur-Mamba-N2\")\n",
    "\n",
    "History=[]\n",
    "for epoch in range(0,400,1):\n",
    "    count=0\n",
    "    db_train = db_train.shuffle(16)\n",
    "    Average_loss=0\n",
    "    for batch_size in db_train:\n",
    "        count+=1\n",
    "        train_image = []\n",
    "        for i in range(len(batch_size)):\n",
    "            tem=load_image(batch_size[i])\n",
    "            train_image.append(load_image(batch_size[i]))\n",
    "        train_image=np.array(train_image,dtype=float)\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            small_size = random.choice([10,20,40])\n",
    "            t = diffusion.sample_timesteps(n=train_image.shape[0],small_size=small_size,step=Step)\n",
    "            x_t = diffusion.noise_images(train_image, t, ,step=Step, size=small_size)#t时刻噪声图\n",
    "            \n",
    "            predicted_noise_image = unet(x_t, t) #t-1时刻的噪声\n",
    "\n",
    "            #loss = multi_gpu_loss(loss, global_batch_size=BATCH_SIZE)\n",
    "            loss = tf.keras.losses.mean_absolute_error(train_image,predicted_noise_image)\n",
    "            loss = tf.reduce_mean(loss)\n",
    "            \n",
    "            gradients = tape.gradient(loss, unet.trainable_variables)\n",
    "            optimizer.apply_gradients(zip(gradients, unet.trainable_variables))\n",
    "            #tf.print(\"Loss:%4.2f\" %(loss))   \n",
    "            Average_loss=Average_loss + loss\n",
    "            \n",
    "    ###valid###\n",
    "    if epoch%20==0:\n",
    "        small_size = random.choice([10,20,40])\n",
    "        t = diffusion.sample_timesteps(n=train_image.shape[0],small_size=small_size,step=Step)\n",
    "        x_t = diffusion.noise_images(train_image, t, step=Step, size=small_size)#t时刻噪声图\n",
    "        plt.imshow(x_t[0])\n",
    "        plt.show()\n",
    "        predicted_noise_image = unet(x_t, t) #t-1时刻的噪声\n",
    "        plt.imshow(predicted_noise_image[0])\n",
    "        plt.show()\n",
    "\n",
    "    Average_loss = Average_loss/count\n",
    "    History.append([epoch, Average_loss])\n",
    "    tf.print(\"=>Epoch%4d  Averageloss:%4.2f\" %(epoch, Average_loss))   \n",
    "\n",
    "    unet.save_weights(\"./Diffusion-Paints-blur-Mamba-N2\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
